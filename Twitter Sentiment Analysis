{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3428111,"sourceType":"datasetVersion","datasetId":2066095}],"dockerImageVersionId":30558,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom transformers import ElectraTokenizer, ElectraForSequenceClassification, AdamW\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\n\n# Load your dataset into a Pandas DataFrame with two columns: 'text' and 'sentiment'.\ndf = pd.read_csv('/kaggle/input/twitter-tweets-sentiment-dataset/Tweets.csv')\ndf = df.head(5000)\n# Preprocessing: Remove missing values and apply regular expression substitutions.\ndf['text'] = df['text'].fillna('').str.replace(r'[:;=]-?[\\)D\\(\\[\\]/\\\\OpP]', '').str.replace(r'[^a-zA-Z\\s]', '')\n\n# Split the data into training and testing sets.\nX_train, X_test, y_train, y_test = train_test_split(df['text'], df['sentiment'], test_size=0.2, random_state=42)\n\n# Encode labels using LabelEncoder\nlabel_encoder = LabelEncoder()\ny_train_encoded = label_encoder.fit_transform(y_train)\ny_test_encoded = label_encoder.transform(y_test)\n\n# Load Electra tokenizer and model\ntokenizer = ElectraTokenizer.from_pretrained(\"google/electra-small-discriminator\")\nmodel = ElectraForSequenceClassification.from_pretrained(\"google/electra-small-generator\", num_labels=len(label_encoder.classes_))\n\n# Tokenize and pad sequences\nmax_sequence_length = 128  # Adjust the sequence length as needed\nX_train_sequences = tokenizer(list(X_train), padding='max_length', max_length=max_sequence_length, truncation=True, return_tensors='pt', return_attention_mask=True)\nX_test_sequences = tokenizer(list(X_test), padding='max_length', max_length=max_sequence_length, truncation=True, return_tensors='pt', return_attention_mask=True)\n\n# Prepare labels as tensors\ny_train_tensors = torch.tensor(y_train_encoded)\ny_test_tensors = torch.tensor(y_test_encoded)\n\n# Create DataLoader for training and validation sets\ntrain_data = TensorDataset(X_train_sequences.input_ids, X_train_sequences.attention_mask, y_train_tensors)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=64)\n\nvalidation_data = TensorDataset(X_test_sequences.input_ids, X_test_sequences.attention_mask, y_test_tensors)\nvalidation_sampler = SequentialSampler(validation_data)\nvalidation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=64)\n\n# Set optimizer and loss function\noptimizer = AdamW(model.parameters(), lr=1e-5)\nmodel.train()\n\n# Train for a few epochs (e.g., 3)\nfor epoch in range(3):\n    for batch in train_dataloader:\n        optimizer.zero_grad()\n        inputs = {'input_ids': batch[0], 'attention_mask': batch[1]}\n        labels = batch[2]\n        outputs = model(**inputs, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n\n# Evaluate the model on the test data\nmodel.eval()\npredictions = []\nwith torch.no_grad():\n    for batch in validation_dataloader:\n        inputs = {'input_ids': batch[0], 'attention_mask': batch[1]}\n        labels = batch[2]\n        outputs = model(**inputs)\n        logits = outputs.logits\n        predictions.extend(logits.argmax(dim=1).cpu().numpy())\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test_encoded, predictions)\nprint(f'Accuracy: {accuracy * 100:.2f}%')\n","metadata":{"execution":{"iopub.status.busy":"2023-11-04T12:10:10.517961Z","iopub.execute_input":"2023-11-04T12:10:10.518349Z","iopub.status.idle":"2023-11-04T12:27:28.636770Z","shell.execute_reply.started":"2023-11-04T12:10:10.518319Z","shell.execute_reply":"2023-11-04T12:27:28.635491Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-generator and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Accuracy: 72.10%\n","output_type":"stream"}]}]}